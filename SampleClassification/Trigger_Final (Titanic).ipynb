{"cells":[{"cell_type":"markdown","source":["**PROBLEM STATEMENT**\n<br/>Predict the House price depending on depending on the date of purchase, distance from local institutes, location etc.\n<br/>Get Sample data from Source- https://data.world/nrippner/titanic-disaster-dataset\n<br/>\n<br/>**COLUMN DEFINITION**\n<br/>survival - Survival (0 = No; 1 = Yes)\n<br/>class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n<br/>name - Name\n<br/>sex - Sex (Male, Female the dataset is Imbalanced towards Males)\n<br/>age - Age\n<br/>\n<br/>**STEPS IN MODELLING**\n<br/>1.Data Acquisation\n<br/>2.Data understanding\n<br/>3.Data visualisation/EDA\n<br/>4.Data cleaning/missing imputation/typecasting\n<br/>5.Sampling/ bias removal\n<br/>6.Anomaly detection\n<br/>7.Feature selection/importance\n<br/>8.Azure ML Model trigger\n<br/>9.Model Interpretation & Error Analysis\n<br/>10.Telemetry\n<br/>\n<br/>**FEATURE ENGINEERING**\n<br/>1. Data is Imbalanced with more Males, so Cluster Oversample by 'Sex' Column and then model. This imbalance can be identified via the Data Plots."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4fa5ca6-5a92-41f6-b17a-2859005a0814"}}},{"cell_type":"markdown","source":["## Import functions from Master Notebook:\nImport the Functions and dependencies from the Master notebook to be used in the Trigger Notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa0d1a0a-4f01-4915-a712-21c6331ee9c2"}}},{"cell_type":"code","source":["%run /Users/.../AMLMasterNotebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f13e839-4cb7-40e6-aba4-e9ec0c6673d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 1.Data Acquisition\n1.Acquisition of data from datasource ADLS path in CSV/Parquet/JSON etc format.\n<br/>2.Logical Transformations in data. \n<br/>3.Transforming columns into required datatypes, converting to pandas df, persisiting actual dataset, intoducing a column 'Index' to assign a unique identifier to each dataset row so that this canm be used to retrieve back the original form after any data manupulations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2142e726-9ed2-4036-887f-dabcf7da3d88"}}},{"cell_type":"code","source":["%scala\n//<USER INPUT FILEPATH PARQUET OR CSV>\n\nval filepath= \"adl://<Your Datalake storage>.azuredatalakestore.net/Temp/ML-PJC/Titanic.csv\"\nvar df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\").load(filepath)\n//val filepath =\"abfss:/.../.parquet\"\n//var df = spark.read.parquet(filepath)\ndf.createOrReplaceTempView(\"vw\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Data from Lake","showTitle":true,"inputWidgets":{},"nuid":"fc14c885-a8e5-41d1-824c-87f70d2dacc9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Logical transformation of data","showTitle":true,"inputWidgets":{},"nuid":"e5dccaea-64b2-450c-b87b-3ce62183c494"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import col\n\ninput_dataframe= spark.sql(\"\"\"select * FROM vw\"\"\")\n#input_dataframe = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\n\ncols_string=['Name','PClass','Sex']\ncols_int=['Age','Survived']\ncols_datetime=[]\ncols_Float=[]\t\t\t\t\n\n#Function call: DataTypeConversion(input_dataframe,cols_string,cols_int,cols_datetime,cols_Float)\ninput_dataframe = DataTypeConversion(input_dataframe,cols_string,cols_int,cols_datetime,cols_Float)\n\n##To assign an Index unique identifier of original record from after data massaging\ninput_dataframe['Index'] = np.arange(len(input_dataframe)) \n\n#Saving data acquired in dbfs for future use\noutdir = '/dbfs/FileStore/Titanic.csv'\ninput_dataframe.to_csv(outdir, index=False)\n#input_dataframe = pd.read_csv(\"/dbfs/FileStore/Dataframe.csv\", header='infer')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data columns and structural transformation","showTitle":true,"inputWidgets":{},"nuid":"14453f67-a88e-46f2-a9ef-066cd8a2524d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.Data Exploration\n1.Exploratory Data Analysis (EDA)- To understand the overall data at hand, analysing each feature independently for its' statistics, the correlation and interraction between variables, data sample etc. \n<br/>2.Data Profiling Plots- To analyse the Categorical and Numerical columns separately for any trend in data, biasness in data etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f0abc17-5ceb-433f-a706-9bbd111b434e"}}},{"cell_type":"code","source":["input_dataframe = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\n\n#Function Call: Data_Profiling_viaPandasProfiling(input_dataframe)\np=Data_Profiling_viaPandasProfiling(input_dataframe)\ndisplayHTML(p)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"EDA","showTitle":true,"inputWidgets":{},"nuid":"29b65c27-980f-4981-bf45-0b6e124a92c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["input_dataframe = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\n\n#User Inputs\ncols_all=['Name','PClass','Sex','Age','Survived']\nCategorical_cols=['Name','PClass','Sex']\nNumeric_cols=['Age','Survived']\nLabel_col='Survived'\n\n#Data_Profiling_Plots(input_dataframe,Categorical_cols,Numeric_cols,Label_col)\nData_Profiling_Plots(input_dataframe,Categorical_cols,Numeric_cols,Label_col)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Profiling Plots","showTitle":true,"inputWidgets":{},"nuid":"4a7436e3-359c-4de5-9beb-ea726b6e0762"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4.Cleansing\nTo clean the data from NULL values, fix structural errors in columns, drop empty columns, encode the categorical values, normalise the data to bring to the same scale. We also check the Data Distribution via Correlation heatmap of original input dataset v/s the Cleansed dataset to validate whether or not the transformations hampered the original data trend/density."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e5b3f56-5fab-453f-8923-5c1afc0a1253"}}},{"cell_type":"code","source":["subsample_final = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\nfilepath=\"/dbfs/FileStore/Titanic.csv\"\n#subsample_final=subsample_final.drop(['Index'], axis = 1) # Index is highest variability column hence always imp along PC but has no business value. You can append columns to be dropped by your choice here in the list\n\n\ninputdf_new=autodatacleaner(subsample_final,filepath,\"Titanic\",\"Data Cleanser\")\nprint(\"Total rows in the new pandas dataframe:\",len(inputdf_new.index))\n\n#persist cleansed data sets \nfilepath1 = '/dbfs/FileStore/Cleansed_Titanic.csv'\ninputdf_new.to_csv(filepath1, index=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"3.Auto Cleanser ","showTitle":true,"inputWidgets":{},"nuid":"08f53513-9939-462b-bd37-97cdec8f8349"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["original = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\ndisplay(Data_Profiling_Fin(original))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data profiling(Heatmap correlation)- User input dataframe","showTitle":true,"inputWidgets":{},"nuid":"eac62f13-7466-4cdb-83e8-9e52ea76f7d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["Cleansed=pd.read_csv(\"/dbfs/FileStore/Cleansed_Titanic.csv\", header='infer')\n\ndisplay(Data_Profiling_Fin(Cleansed))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data profiling(Heatmap correlation)- Cleansed dataframe","showTitle":true,"inputWidgets":{},"nuid":"567edbb4-073f-4056-b8a2-689e93b94dca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4.Sampling\nPerform Stratified, Systematic, Random, Cluster sampling over data and compare the so obtained sampled dataset with the original data using a NULL Hypothesis, and suggest the best sample obtained thus. Compare the data densities of sampled datasets with that of the original input dataset to validate that our sample matches the data trend of original set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2063c713-e44f-46fc-8ecb-0fa99316f808"}}},{"cell_type":"code","source":["input_dataframe = pd.read_csv(\"/dbfs/FileStore/Cleansed_Titanic.csv\", header='infer') ## Sample after cleansing so that all categorical cols converted to num and hence no chi test. chi test requires the total of observed and tot of original sample to be same in frequency. \nfilepath=\"/dbfs/FileStore/Cleansed_Titanic.csv\"\nsubsample_final = pd.DataFrame()\nsubsample1 = pd.DataFrame()\nsubsample2 = pd.DataFrame()\nsubsample3 = pd.DataFrame()\nsubsample4 = pd.DataFrame()\n\n#Function Call: Sampling(input_dataframe,filepath,task_type,input_appname,cluster_classified_col_ifany(Supervised))\nsubsample_final,subsample1,subsample2,subsample3,subsample4=Sampling(input_dataframe,filepath,'Sampling','Titanic','Sex')\n\n#persist sampled data sets \nfilepath1 = '/dbfs/FileStore/StratifiedSampled_Titanic.csv'\nsubsample1.to_csv(filepath1, index=False)\nfilepath2 = '/dbfs/FileStore/RandomSampled_Titanic.csv'\nsubsample2.to_csv(filepath2, index=False)\nfilepath3 = '/dbfs/FileStore/SystematicSampled_Titanic.csv'\nsubsample3.to_csv(filepath3, index=False)\nfilepath4 = '/dbfs/FileStore/ClusterSampled_Titanic.csv'\nsubsample4.to_csv(filepath4, index=False)\nfilepath = '/dbfs/FileStore/subsample_final_Titanic.csv'\nsubsample_final.to_csv(filepath, index=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Sampling ","showTitle":true,"inputWidgets":{},"nuid":"63991afa-17f9-4354-81d9-547d10606f29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["original = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\n\ndisplay(display_DataDistribution(original,'Survived'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Distribution (Histogram)- Input dataset","showTitle":true,"inputWidgets":{},"nuid":"df43a70e-78aa-405b-af9c-e5f6c93f6a4c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["subsample1 = pd.read_csv(\"/dbfs/FileStore/StratifiedSampled_Titanic.csv\", header='infer')\n\ndisplay(display_DataDistribution(subsample1,'Survived'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Distribution (Histogram)- Stratified Sampled dataset","showTitle":true,"inputWidgets":{},"nuid":"8bc0d99e-9afe-4baa-8f24-ff7a008b416d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["subsample2 = pd.read_csv(\"/dbfs/FileStore/RandomSampled_Titanic.csv\", header='infer')\n\ndisplay(display_DataDistribution(subsample2,'Survived'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Distribution (Histogram)- Random Sampled dataset","showTitle":true,"inputWidgets":{},"nuid":"1b8865f1-54ce-4884-95be-d042bdf304d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["subsample3 = pd.read_csv(\"/dbfs/FileStore/SystematicSampled_Titanic.csv\", header='infer')\n\ndisplay(display_DataDistribution(subsample3,'Survived'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Distribution (Histogram)- Systematic Sampled dataset","showTitle":true,"inputWidgets":{},"nuid":"f78cd2a3-564c-42bc-8ee5-14230889bc3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["subsample4 = pd.read_csv(\"/dbfs/FileStore/ClusterSampled_Titanic.csv\", header='infer')\n\ndisplay(display_DataDistribution(subsample4,'Survived'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Distribution (Histogram)- Cluster Sampled dataset","showTitle":true,"inputWidgets":{},"nuid":"1a3087f7-77c2-4fe4-96bc-8bc846a7ed62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5.Anomaly Detection\nIterate data over various Anomaly-detection techniques and estimate the number of Inliers and Outliers for each."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f22e793-74b7-4822-b53c-224c1470250e"}}},{"cell_type":"code","source":["#Calling the Anamoly Detection Function for identifying outliers  \noutliers_fraction = 0.05\n#df =pd.read_csv(\"/dbfs/FileStore/subsample_final_Titanic.csv\", header='infer')\ndf =pd.read_csv(\"/dbfs/FileStore/ClusterSampled_Titanic.csv\", header='infer')\ntarget_variable = 'Survived'\nvariables_to_analyze='Sex'\n\nAnomalyDetection(df,target_variable,variables_to_analyze,outliers_fraction,'anomaly_test','Titanic')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"5.Anomaly Detection","showTitle":false,"inputWidgets":{},"nuid":"1e4e84be-9502-4a62-b9b6-a48a399cffd9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 6.Feature Selection\nPerform feature selection on the basis of Feature Importance ranking, correlation values, variance within the column.\nChoose features with High Importance value score, drop one of the two highly correlated features, drop features which offer zero variability to data and thus do not increase the entropy of dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58536e06-1dda-4ffd-9953-903c6ac440c4"}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\n#input_dataframe = pd.read_csv(\"/dbfs/FileStore/RealEstate.csv\", header='infer')\n#label_col='Y house price of unit area'\n#filepath=\"/dbfs/FileStore/RealEstate.csv\"\n#input_appname='RealEstate'\n#task_type='FeatureSelectionCleansing'\n#Y_discrete='Continuous'\n\ninput_dataframe = pd.read_csv(\"/dbfs/FileStore/Cleansed_Titanic.csv\", header='infer')\nlabel_col='Survived'\nfilepath='/dbfs/FileStore/Cleansed_Titanic.csv'\ninput_appname='Titanic'\ntask_type='FeatureSelectionCleansing'\nY_discrete='Categorical'\n\nFeatureSelection(input_dataframe,label_col,Y_discrete,filepath,input_appname,task_type)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Selection","showTitle":false,"inputWidgets":{},"nuid":"9734df4c-e84a-41ec-9be4-945a42b472c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 7.Auto ML Trigger - after preprocessing\nTrigger Azure auto ML, pick the best model so obtained and use it to predict the label column. Calculate the Weighted Absolute Accuracy amd push to telemetry. also obtain the data back in original format by using the unique identifier of each row 'Index' and report Actual v/s Predicted Columns. We also provide the direct link to the azure Portal Run for the current experiment for users to follow."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68333ed8-5b9a-4065-9780-b0241ba2da95"}}},{"cell_type":"code","source":["import pandas as pd\ndfclean = pd.read_csv(\"/dbfs/FileStore/Cleansed_Titanic.csv\", header='infer')\n\n#AutoMLFunc(subscription_id,resource_group,workspace_name,input_dataframe,label_col,task_type,input_appname)\ndf=AutoMLFunc('3ecb9b6a-cc42-4b0a-9fd1-6c08027eb201','psbidev','psdatainsightsML',dfclean,'Survived','classification','Titanic')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"7.Auto ML Trigger - after preprocessing","showTitle":true,"inputWidgets":{},"nuid":"7cb25d83-c2f9-4324-b153-9ccbbeccf86e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["##df has just index,y actual, y predicted cols, as rest all cols are encoded after manipulation\nfor col in df.columns:\n  if col not in [\"y_predict\",\"y_actual\",\"Index\"]: \n    df.drop([col], axis=1, inplace=True)\n    \n#dataframe is the actual input dataset     \ndataframe = pd.read_csv(\"/dbfs/FileStore/Titanic.csv\", header='infer')\n\n#Merging Actual Input dataframe with AML output df using Index column\ndataframe_fin = pd.merge(left=dataframe, right=df, left_on='Index', right_on='Index')\ndataframe_fin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"8.Obtain back data in original format after modelling-Classification","showTitle":true,"inputWidgets":{},"nuid":"3beeeebc-6152-428c-8ac9-f97e383a256a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 9.Model Interpretation, Feature Importance, Error Analysis\nWe can explore the model by splitting the Model metrics over various cohorts and analyse the data and model performance for each subclass.We can also get Global & Local feature Importance values for the Model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20fe2d90-e6ab-4339-8698-0b5fe6f64045"}}},{"cell_type":"code","source":["df = pd.read_csv(\"/dbfs/FileStore/Cleansed_Titanic.csv\", header='infer')\nlabel_col='Survived'\nsubscription_id='3ecb9b6a-cc42-4b0a-9fd1-6c08027eb201'\nresource_group='psbidev'\nworkspace_name='psdatainsightsML'\nrun_id='AutoML_45a82620-d605-4643-8a1b-8055e32ffd9b'\niteration=1\ntask='classification'\n\nModelInterpret(df,label_col,subscription_id,resource_group,workspace_name,run_id,iteration,task)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Model interpretation","showTitle":true,"inputWidgets":{},"nuid":"53cd1b9b-73bc-4337-ad50-46818466128e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["df = pd.read_csv(\"/dbfs/FileStore/Cleansed_Titanic.csv\", header='infer')\nlabel_col='Survived'\nsubscription_id='3ecb9b6a-cc42-4b0a-9fd1-6c08027eb201'\nresource_group='psbidev'\nworkspace_name='psdatainsightsML'\nrun_id='AutoML_45a82620-d605-4643-8a1b-8055e32ffd9b'\niteration=1\ntask='classification'\n\nErrorAnalysisDashboard(df,label_col,subscription_id,resource_group,workspace_name,run_id,iteration,task)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Error Analysis","showTitle":true,"inputWidgets":{},"nuid":"ff938679-00d0-4865-92d3-173856d78571"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Trigger_Final (Titanic) (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":170239936553676}},"nbformat":4,"nbformat_minor":0}
